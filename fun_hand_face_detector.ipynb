{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2099668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23988bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mediapipe as mp\n",
    "# from mediapipe.tasks import python\n",
    "# from mediapipe.tasks.python import vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc7be75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = 'C:/Users/ayush/Desktop/hand_landmarker.task'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17796420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mediapipe as mp\n",
    "\n",
    "# BaseOptions = mp.tasks.BaseOptions\n",
    "# HandLandmarker = mp.tasks.vision.HandLandmarker\n",
    "# HandLandmarkerOptions = mp.tasks.vision.HandLandmarkerOptions\n",
    "# HandLandmarkerResult = mp.tasks.vision.HandLandmarkerResult\n",
    "# VisionRunningMode = mp.tasks.vision.RunningMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b53c53fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a hand landmarker instance with the live stream mode:\n",
    "# def print_result(result: HandLandmarkerResult, output_image: mp.Image, timestamp_ms: int):\n",
    "#     print('hand landmarker result: {}'.format(result))\n",
    "\n",
    "# options = HandLandmarkerOptions(\n",
    "#     base_options=BaseOptions(model_asset_path=model_path),\n",
    "#     running_mode=VisionRunningMode.LIVE_STREAM,\n",
    "#     result_callback=print_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c5084b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# landmarker = HandLandmarker.create_from_options(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4487eb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import datetime\n",
    "\n",
    "# # Get the current time\n",
    "# start_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caf33924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mediapipe import solutions\n",
    "# from mediapipe.framework.formats import landmark_pb2\n",
    "# import numpy as np\n",
    "\n",
    "# MARGIN = 10  # pixels\n",
    "# FONT_SIZE = 4\n",
    "# FONT_THICKNESS = 3\n",
    "# HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
    "\n",
    "# # def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "# #   hand_landmarks_list = detection_result.hand_landmarks\n",
    "# #   handedness_list = detection_result.handedness\n",
    "# #   annotated_image = np.copy(rgb_image)\n",
    "\n",
    "# #   # Loop through the detected hands to visualize.\n",
    "# #   for idx in range(len(hand_landmarks_list)):\n",
    "# #     hand_landmarks = hand_landmarks_list[idx]\n",
    "# #     handedness = handedness_list[idx]\n",
    "\n",
    "# #     # Draw the hand landmarks.\n",
    "# #     hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "# #     hand_landmarks_proto.landmark.extend([\n",
    "# #       landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "# #     ])\n",
    "# #     solutions.drawing_utils.draw_landmarks(\n",
    "# #       annotated_image,\n",
    "# #       hand_landmarks_proto,\n",
    "# #       solutions.hands.HAND_CONNECTIONS,\n",
    "# #       solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "# #       solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "# #     # Get the top left corner of the detected hand's bounding box.\n",
    "# #     height, width, _ = annotated_image.shape\n",
    "# #     x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "# #     y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "# #     text_x = int(min(x_coordinates) * width)\n",
    "# #     text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "\n",
    "# #     # Draw handedness (left or right hand) on the image.\n",
    "# #     cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
    "# #                 (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "# #                 FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "# #   return annotated_image\n",
    "\n",
    "# def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "#     hand_landmarks_list = detection_result.hand_landmarks\n",
    "#     handedness_list = detection_result.handedness\n",
    "#     annotated_image = np.copy(rgb_image)\n",
    "\n",
    "#     height, width, _ = annotated_image.shape\n",
    "\n",
    "#     # Loop through the detected hands to visualize.\n",
    "#     for idx in range(len(hand_landmarks_list)):\n",
    "#         hand_landmarks = hand_landmarks_list[idx]\n",
    "#         handedness = handedness_list[idx]\n",
    "\n",
    "#         # Draw the hand landmarks.\n",
    "#         hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "#         hand_landmarks_proto.landmark.extend([\n",
    "#             landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "#         ])\n",
    "#         mp.solutions.drawing_utils.draw_landmarks(\n",
    "#             annotated_image,\n",
    "#             hand_landmarks_proto,\n",
    "#             mp.solutions.hands.HAND_CONNECTIONS,\n",
    "#             mp.solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "#             mp.solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "#         # Get the top left corner of the detected hand's bounding box.\n",
    "#         x_coordinates = [landmark.x * width for landmark in hand_landmarks]\n",
    "#         y_coordinates = [landmark.y * height for landmark in hand_landmarks]\n",
    "#         text_x = int(min(x_coordinates))\n",
    "#         text_y = int(min(y_coordinates)) - MARGIN\n",
    "\n",
    "#         # Draw handedness (left or right hand) on the image.\n",
    "#         cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
    "#                     (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "#                     FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "#     return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba639beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eed2d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture = cv2.VideoCapture(0)\n",
    "\n",
    "# # from time import gmtime, strftime\n",
    "# # current_time = strftime(\"%H:%M:%S\", gmtime())\n",
    "\n",
    "\n",
    "# while True :\n",
    "#     returnvalue , frame = capture.read()\n",
    "#     frame = cv2.flip(frame,100)\n",
    "#     if(returnvalue==True):\n",
    "        \n",
    "#         end_time = datetime.datetime.now()\n",
    "#         time_diff = end_time - start_time\n",
    "#         # Convert to milliseconds\n",
    "#         msec = int(time_diff.total_seconds() * 1000)\n",
    "        \n",
    "#         mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "#         detection_result = landmarker.detect_async(mp_image,timestamp_ms=msec) # , frame_timestamp_ms=10\n",
    "#         if detection_result is not None:\n",
    "#             annotated_image = draw_landmarks_on_image(mp_image.numpy_view(), detection_result)\n",
    "#             cv2.imshow(\"webcam feed \",annotated_image)\n",
    "#         else:\n",
    "#             print(\"No hand detected in the frame\")\n",
    "#             cv2.imshow(\"webcam feed \",frame)\n",
    "            \n",
    "#         # annotated_image = draw_landmarks_on_image(mp_image.numpy_view(), detection_result)\n",
    "#         # the video display stops when the q button is pressed , standard opencv way to stop the video\n",
    "#         if cv2.waitKey(5) == ord('q'):\n",
    "#             break\n",
    "#         # cv2.imshow(\"webcam feed \",annotated_image)\n",
    "#     else :\n",
    "#         print(\"ERROR in loading the video \")\n",
    "# capture.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f2a4889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbb8ca4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda_installed_22_12_23\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_face_detection.FaceDetection(\n",
    "    model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(image)\n",
    "\n",
    "    # Draw the face detection annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.detections:\n",
    "      for detection in results.detections:\n",
    "        mp_drawing.draw_detection(image, detection)\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Face Detection', cv2.flip(image, 1))\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81dfc30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda_installed_22_12_23\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as holistic:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(image)\n",
    "\n",
    "    # Draw landmark annotation on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.face_landmarks,\n",
    "        mp_holistic.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_holistic.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles\n",
    "        .get_default_pose_landmarks_style())\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Holistic', cv2.flip(image, 1))\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bdfd590",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.core.multiarray.flagsobj' object has no attribute 'writable'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread() \n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Make detections \u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m image, results \u001b[38;5;241m=\u001b[39m mediapipe_detection(frame, holistic) \n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(results) \n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Draw landmarks \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m, in \u001b[0;36mmediapipe_detection\u001b[1;34m(image, model)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmediapipe_detection\u001b[39m(image, model): \n\u001b[0;32m     10\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB) \u001b[38;5;66;03m# COLOR CONVERSION BGR 2 RGB \u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwritable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m                  \u001b[38;5;66;03m# Image is no longer writable \u001b[39;00m\n\u001b[0;32m     12\u001b[0m     results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mprocess(image)                 \u001b[38;5;66;03m# Make prediction \u001b[39;00m\n\u001b[0;32m     13\u001b[0m     image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwritable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m                   \u001b[38;5;66;03m# Image is now writable  \u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.core.multiarray.flagsobj' object has no attribute 'writable'"
     ]
    }
   ],
   "source": [
    "# Import packages \n",
    "import cv2 \n",
    "import mediapipe as mp \n",
    "  \n",
    "#Build Keypoints using MP Holistic \n",
    "mp_holistic = mp.solutions.holistic # Holistic model \n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities \n",
    "  \n",
    "def mediapipe_detection(image, model): \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB \n",
    "    image.flags.writable = False                  # Image is no longer writable \n",
    "    results = model.process(image)                 # Make prediction \n",
    "    image.flags.writable = True                   # Image is now writable  \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR CONVERSION RGB 2 BGR \n",
    "    return image, results \n",
    "    \n",
    "def draw_landmarks(image, results): \n",
    "    mp_drawing.draw_landmarks( \n",
    "      image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS) # Draw face connections \n",
    "    mp_drawing.draw_landmarks( \n",
    "      image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections \n",
    "    mp_drawing.draw_landmarks( \n",
    "      image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections \n",
    "    mp_drawing.draw_landmarks( \n",
    "      image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections \n",
    "      \n",
    "def draw_styled_landmarks(image, results): \n",
    "    # Draw face connections \n",
    "    mp_drawing.draw_landmarks( \n",
    "      image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS, \n",
    "      mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),  \n",
    "      mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))  \n",
    "    # Draw pose connections \n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),  \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2) \n",
    "                             )  \n",
    "    # Draw left hand connections \n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,  \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),  \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2) \n",
    "                             )  \n",
    "    # Draw right hand connections   \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,  \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),  \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                             )  \n",
    "#Main function \n",
    "cap = cv2.VideoCapture(0) \n",
    "# Set mediapipe model  \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic: \n",
    "    while cap.isOpened(): \n",
    "  \n",
    "        # Read feed \n",
    "        ret, frame = cap.read() \n",
    "  \n",
    "        # Make detections \n",
    "        image, results = mediapipe_detection(frame, holistic) \n",
    "        print(results) \n",
    "          \n",
    "        # Draw landmarks \n",
    "        draw_styled_landmarks(image, results) \n",
    "  \n",
    "        # Show to screen \n",
    "        cv2.imshow('OpenCV Feed', image) \n",
    "  \n",
    "        # Break gracefully \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'): \n",
    "            break\n",
    "    cap.release() \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bc1801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
